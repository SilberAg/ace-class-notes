{"cells":[{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\nimport matplotlib as plot\nimport seaborn as sns\nimport sklearn\n\n# Input data files are available in the \"../input/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# Any results you write to the current directory are saved as output.","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Importing data"},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true},"cell_type":"code","source":"# Importing data\ntest_data = pd.read_csv(\"/kaggle/input/ace-class-assignment/Test.csv\", header=0, sep=\",\")\namp_train = pd.read_csv(\"/kaggle/input/ace-class-assignment/AMP_TrainSet.csv\", header=0, sep=\",\")\nprint(amp_train)\n#print(test)\n\n\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"amp_train.columns","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#test_amp =amp_train[['FULL_Charge', 'FULL_AcidicMolPerc','FULL_OOBM850104','FULL_DAYM780201', 'AS_MeanAmphiMoment','CLASS']]\n#print(test_amp)\n#test_data=test_data[['FULL_Charge', 'FULL_AcidicMolPerc','FULL_OOBM850104','FULL_DAYM780201', 'AS_MeanAmphiMoment']]\n#X=test_amp.drop(\"CLASS\",axis=1)\n#Y=test_amp.CLASS\n#model=GaussianNB()   \n#model.fit(X,Y)\n#kj_results=model.predict(test_data)\n\n#print(test_data2)\n#kj_results","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## General data attributes and descriptive statistics"},{"metadata":{"trusted":true},"cell_type":"code","source":"#Describing general data attributes\ntrain_dim=amp_train.shape #gets the dimensions of the tranining dataset\ntest_dim= test_data.shape #gets the dimensions of the tesing dataset\nprint(train_dim)\nprint(test_dim)\n\natr_types=amp_train.dtypes #gets the data type for each column in training dataset\nprint(atr_types)\namp_train.isnull().sum() # gets the sum of the missing values/observations in each column\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Descriptive statistics \nstats=amp_train.describe()\nprint(stats)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Class distributions\n\nSometimes, observations from the different classes may not be equal, these may need special handling, so its important to note the class distributions, as if they are uneven, they may bias our model"},{"metadata":{"trusted":true},"cell_type":"code","source":"# CLass distibutions \ncdists= amp_train.groupby('CLASS').size() # groups the observation by the column of class, and counts all the observations including null values\nprint(cdists)\n\n#plots to visualize the class distribution\namp_train.groupby('CLASS').size().plot(kind=\"pie\")","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"The observations from each class seem to be equal"},{"metadata":{},"cell_type":"markdown","source":"# Correlation between the different attributes\nBasically, we need to examine our attributes for correlation as highly correlated attributes present the same data to the algorithm. Removing one may speed learning of the algorithm (less dimensions more speed)"},{"metadata":{"trusted":true},"cell_type":"code","source":"# Correlation between attributes \ncorrelations = amp_train.corr(method=\"pearson\") # calculates pairwaise correlation for the attributes \nprint(correlations)\n\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"FULL_AURR980107 and FULL_AcidicMolPerc have a correlation coefficent of 0.79, this might mean these two attributes may be some what correlated. Same goes for AS_DAYM780201 and FULL_DAYM780201 (correlation coefficient is 0.894191)"},{"metadata":{"trusted":true},"cell_type":"code","source":"# Plots to visualize the correlations\nfrom matplotlib import rcParams\nsns.heatmap(correlations, annot=True)\nrcParams['figure.figsize'] = 11.7,8.27\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Data skewness\n\nMost machine learning algorithms assume that data from the attributes is normally distributed, so identifying attributes with data that has a left or right skew, can be important as this can be easily corrected"},{"metadata":{"trusted":true},"cell_type":"code","source":"sk = amp_train.skew() # calculates the skew for each attribute\nprint(sk)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Negative values like those of FULL_DAYM780201, FULL_OOBM850104,AS_DAYM780201, AS_FUKS010112, indicate that these attributes have values skewed more to the left\nValues closer to zero are indicative of a less or no skew, while positive values would mean that that particular attribute is skewed more to the right"},{"metadata":{},"cell_type":"markdown","source":" # Visualising data distributions with plots"},{"metadata":{"trusted":true},"cell_type":"code","source":"# Plot one, histogram to show the distributions\nimport matplotlib as plot \nfrom matplotlib import pyplot\namp_train.hist(layout=(4,3), figsize=(16,16))\nplot.pyplot.subplots_adjust(wspace=0.4, hspace=0.5)\npyplot.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Plot two density plots can show the distributions better\namp_train.plot(kind=\"density\",subplots=True,layout=(4,3),sharex=False,sharey=False, figsize=(16,16))\n#amp_train.plot(kind=\"density\",subplots=False,layout=(4,3),sharex=True,sharey=True)\npyplot.show()\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Data transformations\nUsing square roots to transform the attributes with negative values into postives"},{"metadata":{"trusted":true},"cell_type":"code","source":"\nneg1=amp_train.iloc[:,0]\nprint(neg1)\ntneg1=neg1**2\ntneg1.plot(kind=\"density\")\namp_train.iloc[:,0]=tneg1\n\n\n\nneg2=amp_train.iloc[:,5]\nprint(neg2)\ntneg2=neg2**2\ntneg2.plot(kind=\"density\")\namp_train.iloc[:,5]=tneg2\n\n## test\nneg3=test_data.iloc[:,0]\nprint(neg3)\ntneg3=neg3**2\ntneg3.plot(kind=\"density\")\ntest_data.iloc[:,0]=tneg3\n\n\n\nneg4=test_data.iloc[:,5]\nprint(neg4)\ntneg4=neg4**2\ntneg4.plot(kind=\"density\")\ntest_data.iloc[:,5]=tneg4","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"distributions for AS_MeanAmphiMoment,FULL_AcidicMolPerc and NT_EFC195 dont look so good"},{"metadata":{},"cell_type":"markdown","source":"# Feature selection\n\n\n"},{"metadata":{"trusted":true},"cell_type":"code","source":" #test one selecting out features with low variance\nfrom sklearn.feature_selection import VarianceThreshold\nsel = VarianceThreshold(threshold=(.8 * (1 - .8)))\nk=sel.fit_transform(amp_train)\nnames=amp_train.columns[sel.get_support(indices=True)]\nprint(k.shape)\namp_train2=pd.DataFrame(k,columns=names)\nprint(amp_train2)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#test two\nfrom sklearn.feature_selection import SelectKBest\nfrom sklearn.feature_selection import chi2\n\ntest_array = amp_train2.values\n\ntest_atr = test_array[:,0:7]\nclass_atr = test_array[:,7]\n\n\ntest = SelectKBest(score_func=chi2, k=5)\nfit = test.fit(test_atr, class_atr)\n\n\nprint(fit.scores_)\nfeatures = fit.transform(test_atr)\nnames2=amp_train2.columns[fit.get_support(indices=True)]\namp_train3=pd.DataFrame(features,columns=names2)\nprint(amp_train3[5:])\n\n\n\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Test three Recursive feature elimination\n\nfrom sklearn.feature_selection import RFE\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.ensemble import  RandomForestClassifier\n\nmodel1 = LogisticRegression()\nmodel2=RandomForestClassifier()\n\nrfe = RFE(model1, 4)\nfit2 = rfe.fit(test_atr, class_atr)\n\nrfe2= RFE(model2, 4)\nfit3 = rfe2.fit(test_atr, class_atr)\n\n\n\n\nnames3=amp_train2.columns[fit2.get_support(indices=True)]\n\nnames4=amp_train2.columns[fit3.get_support(indices=True)]\n\nprint(names3)\nprint(names4)\nprint(names2)\nprint(amp_train.columns)\nprint(amp_train2)\n\n\n\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"test_amp =amp_train.loc[:,['FULL_Charge', 'FULL_AcidicMolPerc',\"FULL_OOBM850104\",'FULL_DAYM780201', 'AS_MeanAmphiMoment','CLASS']]\nprint(test_amp)\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.model_selection import train_test_split\nfrom sklearn.model_selection import KFold\nfrom sklearn.model_selection import cross_val_score\nfrom sklearn.metrics import confusion_matrix\n\n\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.discriminant_analysis import LinearDiscriminantAnalysis\nfrom sklearn.naive_bayes import GaussianNB\nfrom sklearn.svm import SVC\nfrom sklearn.neighbors import KNeighborsClassifier\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Logistic regression using cross validation"},{"metadata":{"trusted":true},"cell_type":"code","source":"\nX=test_amp.iloc[:,0:5]\nY=test_amp.iloc[:,5]\nfolds=10\nkfold = KFold(n_splits=folds, random_state=7, shuffle=True,)\nmodel = LogisticRegression()\nresults = cross_val_score(model, X, Y, cv=kfold)\n\nprint((\"Accuracy: %.3f%% (%.3f%%)\") % (results.mean()*100.0, results.std()*100.0))\n\n\n\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"test_data = pd.read_csv(\"/kaggle/input/ace-class-assignment/Test.csv\", header=0, sep=\",\")\namp_train = pd.read_csv(\"/kaggle/input/ace-class-assignment/AMP_TrainSet.csv\", header=0, sep=\",\")\ntest_amp =amp_train.loc[:,['FULL_Charge', 'FULL_AcidicMolPerc',\"FULL_OOBM850104\",'FULL_DAYM780201', 'AS_MeanAmphiMoment','CLASS']]","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Logistic regression with a test train split"},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.metrics import matthews_corrcoef\ntest_amp=test_amp.values\n#X=test_amp.iloc[:,0:5]\nX=test_amp[:,0:5]\nY=test_amp[:,5]\n\n#Y=test_amp.iloc[:,5]\n#print(Y)\ntest_size = 0.4\nseed = 25\nX_train, X_test, Y_train, Y_test = train_test_split(X, Y, test_size=test_size,random_state=seed, shuffle=True)\nmodel = LogisticRegression()\nmodel.fit(X_train, Y_train)\npredicted = model.predict(X_test)\nmatrix = confusion_matrix(Y_test, predicted)\nresult = model.score(X_test, Y_test)\nprint(\"Accuracy: \",  (result*100.0))\nprint(matrix)\nprint('MCC: ', matthews_corrcoef(model.predict(X_test),Y_test))\n#print(test_data)\n\ntest_data2=test_data.loc[:,['FULL_Charge', 'FULL_AcidicMolPerc','FULL_DAYM780201', 'FULL_OOBM850104', 'AS_MeanAmphiMoment']]\ntest_data2=test_data2.values    \n#model1 = LogisticRegression()    \n#model1.fit(X,Y)\nkj_results=model.predict(test_data2)\n\n#print(test_data2)\nkj_results","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df = pd.DataFrame(kj_results)\ndf.columns = ['CLASS']\ndf.index.name = 'Index'\ndf['CLASS']=df['CLASS'].map({0.0:False,1.0:True})\nprint(df)\n#X=test_amp.iloc[:,0:5]\n#=test_amp.iloc[:,5]\ndf.to_csv(\"kj.csv\")\ndf[\"CLASS\"].unique()\n#print(df.groupby(\"CLASS\").size()[0].sum())\n#print(df.groupby(\"CLASS\").size()[1].sum())\ndf['CLASS'].value_counts()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Gausian/Naive Bayes"},{"metadata":{"trusted":true},"cell_type":"code","source":"model=GaussianNB()\nfrom sklearn.metrics import matthews_corrcoef\ntest_size = 0.4\nseed = 25\nX_train, X_test, Y_train, Y_test = train_test_split(X, Y, test_size=test_size,\nrandom_state=seed)\nmodel.fit(X_train, Y_train)\npredicted = model.predict(X_test)\nmatrix = confusion_matrix(Y_test, predicted)\nresult = model.score(X_test, Y_test)\nprint(\"Accuracy: \",  (result*100.0))\nprint(matrix)\nprint('MCC: ', matthews_corrcoef(model.predict(X_test),Y_test))\n#print(test_data)\n\nX=test_amp.iloc[:,0:5]\nY=test_amp.iloc[:,5]\nmodel=GaussianNB()   \nmodel.fit(X,Y)\nkj_results=model.predict(test_data2)\n\n#print(test_data2)\nkj_results","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Linear Discriminant Analysis"},{"metadata":{"trusted":true},"cell_type":"code","source":"model=LinearDiscriminantAnalysis()\nfolds=10\nkfold = KFold(n_splits=folds, random_state=7, shuffle=True,)\nresults = cross_val_score(model, X, Y, cv=kfold)\nprint((\"Accuracy: %.3f%% (%.3f%%)\") % (results.mean()*100.0, results.std()*100.0))\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Support Vector Machines"},{"metadata":{"trusted":true},"cell_type":"code","source":"model=SVC()\nfolds=10\nkfold = KFold(n_splits=folds, random_state=7, shuffle=True,)\nresults = cross_val_score(model, X, Y, cv=kfold)\nprint((\"Accuracy: %.3f%% (%.3f%%)\") % (results.mean()*100.0, results.std()*100.0))","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}