{"cells":[{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the \"../input/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\n# DATA VISUALIZATION\nimport matplotlib.pyplot as plt\nimport matplotlib.gridspec as gridspec\nimport seaborn as sns\n\n#scikit learn libraries\n#Lineaer Models\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.neural_network import MLPClassifier\nfrom sklearn.linear_model import SGDClassifier\nfrom sklearn.discriminant_analysis import LinearDiscriminantAnalysis\n\n\nfrom sklearn.svm import SVC\nfrom sklearn.naive_bayes import GaussianNB\n# Tree\nfrom sklearn.tree import DecisionTreeClassifier\n\n# sklearn libraries\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.model_selection import train_test_split,KFold,cross_val_score\nfrom sklearn.preprocessing import normalize\nfrom sklearn.metrics import confusion_matrix,accuracy_score,precision_score,recall_score,f1_score,matthews_corrcoef,classification_report,roc_curve\nfrom sklearn.externals import joblib\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.decomposition import PCA\n\n#let's remove the annoying warnings from our cells.\nimport warnings\nwarnings.filterwarnings('ignore')\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# Any results you write to the current directory are saved as output.","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true},"cell_type":"code","source":"## Loading the datasets\n\nTrain = pd.read_csv(\"../input/ace-class-assignment/AMP_TrainSet.csv\")\nTest = pd.read_csv(\"../input/ace-class-assignment/Test.csv\")\n\n## Taking a preview of the data\nTrain.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"## Loking at the dimensions of the dataset\n\nTrain.shape,Test.shape","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"## Looking for misssing and null values\nTrain.isnull().any().sum()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"## Looking the data types of the data\nTrain.dtypes","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"## Looking at the description of the data \nTrain.describe()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Looking at the data:\n* There are two categorical features CLASS and NT_EFC195\n* There is a big difference between the 75th percentile and the maximum values for columns  FULL_Charge and FULL_AcidicMolPerc\n* The columns FULL_AcidicMolPerc and FULL_OOBM850104 have both negative and positive values."},{"metadata":{"trusted":true},"cell_type":"code","source":"# Looking at the ratio of the binary data for the class\n\nAll = Train.shape[0]\nPositive = Train[Train['CLASS'] == 1]\nNegative = Train[Train['CLASS'] == 0]\n\nx = len(Positive)/All\ny = len(Negative)/All\n\nprint('Positives :',x*100,'%')\nprint('Negatives :',y*100,'%')\n\n## The sets are evenly balanced and hence there will be no need to use SMOTE OR NEAR MISS","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Visualizing the data class\nlabels = ['Negatives','Positives']\nclasses = pd.value_counts(Train['CLASS'], sort = True)\nclasses.plot(kind = 'bar', rot=0)\nplt.title(\"Visualizing the data class\")\nplt.xticks(range(2), labels)\nplt.xlabel(\"Class\")\nplt.ylabel(\"Frequency\")","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Plot the distribution of features"},{"metadata":{"trusted":true},"cell_type":"code","source":"# distribution of features\n# This feature works with numerical data not categorical data.\nfeatures =Train.drop([\"CLASS\",\"NT_EFC195\"],axis=1).columns\n\nplt.figure(figsize=(12,12*4))\ngs = gridspec.GridSpec(12, 1)\nfor i, cn in enumerate(Train[features]):\n    ax = plt.subplot(gs[i])\n    sns.distplot(Train[cn][Train.CLASS == 1], bins=50)\n    sns.distplot(Train[cn][Train.CLASS == 0], bins=50)\n    ax.set_xlabel('')\n    ax.set_title('histogram of feature: ' + str(cn))\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# PCA (Principal Component Analysis) mainly using to reduce the size of the feature space while\n# retaining as much of the information as possible.\n# In here all the features transformed into 2 features using PCA.\n\nX = Train.drop(['CLASS'], axis = 1)\ny = Train['CLASS']\n\npca = PCA(n_components=2)\nprincipalComponents = pca.fit_transform(X.values)\nprincipalDf = pd.DataFrame(data = principalComponents\n             , columns = ['principal component 1', 'principal component 2'])\nFTrain = pd.concat([principalDf, y], axis = 1)\nFTrain.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# 2D visualization\nfig = plt.figure(figsize = (8,8))\nax = fig.add_subplot(1,1,1) \nax.set_xlabel('Principal Component 1', fontsize = 15)\nax.set_ylabel('Principal Component 2', fontsize = 15)\nax.set_title('2 component PCA', fontsize = 20)\ntargets = [0, 1]\ncolors = ['r', 'g']\nfor target, color in zip(targets,colors):\n    indicesToKeep = FTrain['CLASS'] == target\n    ax.scatter(FTrain.loc[indicesToKeep, 'principal component 1']\n               , FTrain.loc[indicesToKeep, 'principal component 2']\n               , c = color\n               , s = 50)\nax.legend(targets)\nax.grid()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"\n# Data splitting\n\n# splitting the faeture array and label array keeping 80% for the trainnig sets\nX_train,X_test,y_train,y_test = train_test_split(X,y,test_size=0.20)\n\n# normalize: Scale input vectors individually to unit norm (vector length).\nX_train = normalize(X_train)\nX_test=normalize(X_test)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Spot-Check Algorithms\n\nmodels = []\nmodels.append(('LR', LogisticRegression()))\nmodels.append(('LDA', LinearDiscriminantAnalysis()))\nmodels.append(('KNN', KNeighborsClassifier()))\nmodels.append(('CART', DecisionTreeClassifier()))\nmodels.append(('NB', GaussianNB()))\nmodels.append(('SVM', SVC()))\nmodels.append(('NN',MLPClassifier()))\nmodels.append(('SG',SGDClassifier()))\n\n# evaluate each model in turn\n\nresults = []\nnames = []\nseed =25\nfor name, model in models:\n    kfold = KFold(n_splits=10, random_state=seed,shuffle=True)\n    cv_results = cross_val_score(model, X_train, y_train, cv=kfold, \n    scoring='accuracy')\n    results.append(cv_results)\n    names.append(name)\n    print(\"%s: %f (%f)\" % (name, cv_results.mean(), cv_results.std()))\n#print(msg)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"\n# Model Evaluation\n"},{"metadata":{"trusted":true},"cell_type":"code","source":"\n#scoring knn\nknn_accuracy_score  = accuracy_score(y_test,knn_predicted_test_labels)\nknn_precison_score  = precision_score(y_test,knn_predicted_test_labels)\nknn_recall_score    = recall_score(y_test,knn_predicted_test_labels)\nknn_f1_score        = f1_score(y_test,knn_predicted_test_labels)\nknn_MCC             = matthews_corrcoef(y_test,knn_predicted_test_labels)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"for n in results:\n    print(n)\n#names['LR'].predict(X_test)","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}